{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: PJM Hourly Energy Forecasting with SageMaker DeepAR\n",
    "\n",
    "## Overview\n",
    "In this lab, we will use the **PJM Hourly Energy Consumption Dataset** to predict future electricity demand. We will use **Amazon SageMaker's built-in DeepAR algorithm**, which is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using Recurrent Neural Networks (RNNs).\n",
    "\n",
    "### Lab Objectives:\n",
    "1. **Load and Explore Data**: Read the CSV data and visualize the energy consumption.\n",
    "2. **Preprocess for DeepAR**: Convert the data into the JSON format required by DeepAR.\n",
    "3. **Train a Model**: Use SageMaker Estimators to train a forecasting model.\n",
    "4. **Deploy and Predict**: Deploy an endpoint and visualize predictions against actuals.\n",
    "5. **Reproduce in Canvas**: Learn how to achieve the same results using the No-Code SageMaker Canvas interface.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "First, we import the necessary libraries. We will use `boto3` for AWS services, `sagemaker` for model training, and `pandas`/`matplotlib` for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Set up SageMaker session and role\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/pjm-energy-forecasting'\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data\n",
    "\n",
    "In a real scenario, you would have the full dataset in a folder named `data/hourly_energy_consumption`. \n",
    "\n",
    "**Note:** To ensure this notebook runs for you immediately, the cell below creates a dummy `DAYTON_hourly.csv` file if it doesn't exist. If you have the actual dataset, ensure it is placed in `data/hourly_energy_consumption/DAYTON_hourly.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for data\n",
    "os.makedirs('data/hourly_energy_consumption', exist_ok=True)\n",
    "file_path = 'data/hourly_energy_consumption/DAYTON_hourly.csv'\n",
    "\n",
    "# CHECK: If file doesn't exist, create a synthetic one for the lab to function\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"Dataset not found. Generating sample data for the lab...\")\n",
    "    # Generate date range\n",
    "    dates = pd.date_range(start='2004-12-01', end='2005-02-01', freq='H')\n",
    "    import numpy as np\n",
    "    # Generate synthetic load data (sine wave + random noise)\n",
    "    values = 1500 + 300 * np.sin(np.linspace(0, 100, len(dates))) + np.random.normal(0, 50, len(dates))\n",
    "    df_dummy = pd.DataFrame({'Datetime': dates, 'DAYTON_MW': values.astype(int)})\n",
    "    df_dummy.to_csv(file_path, index=False)\n",
    "    print(f\"Created sample file at {file_path}\")\n",
    "else:\n",
    "    print(f\"Found dataset at {file_path}\")\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Data Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We need to clean the data and visualize it to understand the patterns (seasonality, trends). \n",
    "\n",
    "1. Convert `Datetime` to a proper datetime object.\n",
    "2. Set `Datetime` as the index.\n",
    "3. Handle duplicates or missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert to Datetime\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "# 2. Set Index\n",
    "df = df.set_index('Datetime')\n",
    "\n",
    "# 3. Sort index to ensure chronological order\n",
    "df = df.sort_index()\n",
    "\n",
    "# 4. Plotting a subset\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(df.index, df['DAYTON_MW'], label='Energy Consumption (MW)', color='blue', alpha=0.7)\n",
    "plt.title('Hourly Energy Consumption - Dayton Region')\n",
    "plt.ylabel('Megawatts (MW)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for DeepAR\n",
    "\n",
    "DeepAR requires the data to be in a specific JSON format. We also need to decide on:\n",
    "- **Context Length**: How far back the model looks (e.g., 1 week = 168 hours).\n",
    "- **Prediction Length**: How far forward we predict (e.g., 24 hours).\n",
    "\n",
    "We will split the data into **Training** (past data) and **Testing** (data including the future we want to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'H' # Hourly data\n",
    "prediction_length = 24 # Predict next 24 hours\n",
    "context_length = 24 * 7 # Look back 1 week\n",
    "\n",
    "# Ensure regular frequency (fill missing hours if any)\n",
    "ts_data = df['DAYTON_MW'].resample(freq).mean().fillna(method='ffill')\n",
    "\n",
    "# Split Train and Test\n",
    "# Train ends 24 hours before the end of the dataset\n",
    "train_series = ts_data.iloc[:-prediction_length]\n",
    "test_series = ts_data # Test includes the whole series (DeepAR uses the end to evaluate)\n",
    "\n",
    "print(f\"Train end date: {train_series.index[-1]}\")\n",
    "print(f\"Test end date: {test_series.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert to DeepAR JSON format\n",
    "def write_json_dataset(series, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        # DeepAR expects: start timestamp and target array\n",
    "        json_obj = {\n",
    "            \"start\": str(series.index[0]),\n",
    "            \"target\": list(series.values)\n",
    "        }\n",
    "        f.write(json.dumps(json_obj).encode('utf-8'))\n",
    "        f.write(b'\\n')\n",
    "    print(f\"Created {filename}\")\n",
    "\n",
    "# Create local JSON files\n",
    "write_json_dataset(train_series, 'train.json')\n",
    "write_json_dataset(test_series, 'test.json')\n",
    "\n",
    "# Upload to S3\n",
    "train_path = session.upload_data('train.json', bucket=bucket, key_prefix=f'{prefix}/train')\n",
    "test_path = session.upload_data('test.json', bucket=bucket, key_prefix=f'{prefix}/test')\n",
    "\n",
    "print(f\"Training data uploaded to: {train_path}\")\n",
    "print(f\"Test data uploaded to: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train DeepAR Model\n",
    "\n",
    "We retrieve the built-in DeepAR Docker image and configure the Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve DeepAR Image URI\n",
    "image_uri = image_uris.retrieve(region=region, framework='forecasting-deepar')\n",
    "\n",
    "# Define the Estimator\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge', # Cost-effective instance for training\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Set Hyperparameters\n",
    "estimator.set_hyperparameters(\n",
    "    time_freq=freq,\n",
    "    context_length=str(context_length),\n",
    "    prediction_length=str(prediction_length),\n",
    "    epochs='20',           # Low epochs for quick lab execution\n",
    "    early_stopping_patience='10',\n",
    "    num_layers='2'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training... this may take 5-10 minutes.\")\n",
    "estimator.fit(inputs={'train': train_path, 'test': test_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deploy and Predict\n",
    "\n",
    "Once training is complete, we deploy the model to an endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Deploy the model\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "print(\"Endpoint deployed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "We send the time series data to the endpoint. DeepAR will look at the provided data and forecast the next 24 hours (as defined in `prediction_length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare request\n",
    "# We send the 'train' series. DeepAR will predict the continuation.\n",
    "request_data = {\n",
    "    \"instances\": [\n",
    "        {\"start\": str(train_series.index[0]), \"target\": list(train_series.values)}\n",
    "    ],\n",
    "    \"configuration\": {\"num_samples\": 50, \"output_types\": [\"quantiles\"], \"quantiles\": [\"0.5\", \"0.9\"]}\n",
    "}\n",
    "\n",
    "# Get prediction\n",
    "prediction = predictor.predict(request_data)\n",
    "\n",
    "# Extract forecasts\n",
    "forecast_key = prediction['predictions'][0]\n",
    "p50 = forecast_key['quantiles']['0.5'] # Median prediction\n",
    "p90 = forecast_key['quantiles']['0.9'] # 90th percentile (upper bound)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "forecast_date_range = pd.date_range(start=train_series.index[-1] + pd.Timedelta(hours=1), periods=prediction_length, freq='H')\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Plot actual historical data (Zoom in on last week for visibility)\n",
    "zoom_start = -168 \n",
    "plt.plot(test_series.index[zoom_start:], test_series.values[zoom_start:], label='Actual Data', color='black')\n",
    "\n",
    "# Plot forecast\n",
    "plt.plot(forecast_date_range, p50, label='Predicted (P50)', color='orange', linestyle='--')\n",
    "plt.fill_between(forecast_date_range, p50, p90, color='orange', alpha=0.3, label='Confidence Interval (P90)')\n",
    "\n",
    "plt.title('PJM Energy Forecast: Actual vs DeepAR Prediction')\n",
    "plt.ylabel('MW')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean-up\n",
    "\n",
    "**Important:** Delete the endpoint to avoid incurring charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted.\")\n",
    "\n",
    "# Optional: Remove uploaded data from S3\n",
    "# sagemaker.Session().delete_object(bucket=bucket, key=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Instructions for Reproducing in SageMaker Canvas\n",
    "\n",
    "SageMaker Canvas allows you to build this model without writing any code. Follow these steps to reproduce the results:\n",
    "\n",
    "### Step 1: Import Data\n",
    "1. Open **SageMaker Canvas** from the AWS Console.\n",
    "2. Navigate to **Data** -> **Import**.\n",
    "3. Upload the `DAYTON_hourly.csv` file (you can download it from the `data/` folder in the Jupyter file browser).\n",
    "4. Preview the data and choose **Import**.\n",
    "\n",
    "### Step 2: Build a Model\n",
    "1. Go to **My Models** -> **New Model**.\n",
    "2. Name it `PJM-Energy-Forecast`.\n",
    "3. Select **Predictive Analysis**.\n",
    "\n",
    "### Step 3: Configure Training\n",
    "1. Select your imported dataset.\n",
    "2. **Target Column**: Select `DAYTON_MW`.\n",
    "3. Canvas will automatically detect this is a Time Series problem. If not, click **Configure**.\n",
    "    - **Item ID**: If you had multiple regions in one file, you would select the region name here. Since we only have one, you can leave it blank or add a dummy column.\n",
    "    - **Time Stamp**: Select `Datetime`.\n",
    "    - **Forecast Horizon**: Set to `24` (to match our code).\n",
    "4. Click **Preview model** to see data quality insights.\n",
    "\n",
    "### Step 4: Train\n",
    "1. Click **Standard Build** (takes 1-2 hours but is more accurate) or **Quick Build** (15 mins).\n",
    "2. Wait for training to complete.\n",
    "\n",
    "### Step 5: Analyze and Predict\n",
    "1. Once trained, view the **Overview** tab to see accuracy metrics (wQL, RMSE).\n",
    "2. Go to the **Predict** tab.\n",
    "3. You can generate a **Single Prediction** (what-if analysis) or verify against the held-out test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
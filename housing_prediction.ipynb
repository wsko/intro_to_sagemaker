{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Technical Note for the Student:**\n",
        "\n",
        "* **Kernel:** Select the **`Data Science 3.0`** (or `conda_python3`) kernel when opening this notebook in JupyterLab.\n",
        "* **Permissions:** Ensure your SageMaker Execution Role has `AmazonSageMakerFullAccess` and `AmazonS3FullAccess` (or specific bucket access).\n",
        "\n",
        "---\n",
        "\n",
        "# Lab: California Housing Prediction (SageMaker Edition)\n",
        "\n",
        "**Role:** SageMaker Data Scientist\n",
        "**Objective:** Build an end-to-end ML pipeline to predict median house values. You will wrangle data locally in the notebook, train a model at scale using **AWS Linear Learner** on a separate training cluster, and deploy a real-time inference endpoint.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Environment Setup\n",
        "\n",
        "In SageMaker JupyterLab, the environment is pre-configured with the AWS SDK (`boto3`) and the high-level `sagemaker` library. We initialize our session and define where our data will live in S3."
      ],
      "metadata": {
        "id": "6wC9x4ncDwxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Initialize SageMaker Session\n",
        "session = sagemaker.Session()\n",
        "region = session.boto_region_name\n",
        "\n",
        "# 2. Get Execution Role (The permissions this notebook has)\n",
        "# In SageMaker Studio/JupyterLab, this automatically grabs the role attached to your user profile.\n",
        "role = sagemaker.get_execution_role()\n",
        "\n",
        "# 3. Define S3 Bucket and Prefix\n",
        "# We use the default bucket created by SageMaker for you.\n",
        "bucket = session.default_bucket()\n",
        "prefix = 'labs/california-housing'\n",
        "\n",
        "print(f\"Region: {region}\")\n",
        "print(f\"S3 Bucket: {bucket}\")\n",
        "print(f\"IAM Role: {role.split('/')[-1]}\") # Printing just the role name for readability"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "-34R0heODwxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2. Data Ingestion & Exploration (EDA)\n",
        "\n",
        "We will use `sklearn` to fetch the classic 1990 California Housing dataset."
      ],
      "metadata": {
        "id": "Jj5xQgXKDwx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Download dataset\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "df = data.frame\n",
        "\n",
        "# Quick look at the data\n",
        "print(f\"Dataset Dimensions: {df.shape}\")\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "2f5f2ryYDwx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Visualizing the Geography\n",
        "\n",
        "Since this is geospatial data, plotting the longitude and latitude reveals the map of California."
      ],
      "metadata": {
        "id": "rthpSCo1Dwx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    data=df,\n",
        "    x=\"Longitude\",\n",
        "    y=\"Latitude\",\n",
        "    size=\"Population\",\n",
        "    hue=\"MedHouseVal\",\n",
        "    palette=\"viridis\",\n",
        "    alpha=0.5,\n",
        "    sizes=(10, 200) # Control the size of the dots\n",
        ")\n",
        "plt.title(\"California Housing: Price & Population Density\")\n",
        "plt.legend(title=\"Median House Value\", loc=\"upper right\", bbox_to_anchor=(1.2, 1))\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "_kd_Kaq-Dwx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Correlation Analysis\n",
        "\n",
        "We need to see which features drive housing prices.\n",
        "\n",
        "* **Note:** `MedInc` (Median Income) usually has the strongest correlation with `MedHouseVal`."
      ],
      "metadata": {
        "id": "z5chwlMtDwx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "O1Cr-WPrDwx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Data Wrangling & Feature Engineering\n",
        "\n",
        "SageMaker's Linear Learner expects specific data formats. We will perform the following processing:\n",
        "\n",
        "1. **Feature Engineering:** Create `RoomsPerHousehold` to normalize the room count.\n",
        "2. **Imputation:** (The sklearn version of this dataset is pre-cleaned, but in real scenarios, you would handle `NaNs` here).\n",
        "3. **Splitting:** 70% Train, 15% Validation, 15% Test.\n",
        "4. **Scaling:** Standardize data (Mean , Variance )."
      ],
      "metadata": {
        "id": "aDd__cI8Dwx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Feature Engineering\n",
        "df['RoomsPerHousehold'] = df['AveRooms'] / df['AveOccup']\n",
        "df['BedroomsPerRoom'] = df['AveBedrms'] / df['AveRooms']\n",
        "\n",
        "# 2. Prepare X (Features) and y (Target)\n",
        "X = df.drop(\"MedHouseVal\", axis=1)\n",
        "y = df[\"MedHouseVal\"]\n",
        "\n",
        "# 3. Train / Validation / Test Split\n",
        "# First split: Train (70%) vs Temp (30%)\n",
        "X_train_raw, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Second split: Temp into Validation (15%) and Test (15%)\n",
        "X_val_raw, X_test_raw, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 4. Standardization (Scaling)\n",
        "# Ideally fit on training data only to prevent data leakage\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train_raw)\n",
        "X_val = scaler.transform(X_val_raw)\n",
        "X_test = scaler.transform(X_test_raw)\n",
        "\n",
        "print(f\"Training Data:   {X_train.shape}\")\n",
        "print(f\"Validation Data: {X_val.shape}\")\n",
        "print(f\"Test Data:       {X_test.shape}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UlBNfr5hDwx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Uploading Data to S3\n",
        "\n",
        "When training with SageMaker, the compute instance (the \"training job\") spins up in the background and needs to download data from S3.\n",
        "\n",
        "**Format Requirement:** For CSV input to Linear Learner, the **first column must be the target variable**, and there should be no headers."
      ],
      "metadata": {
        "id": "HDKdxc7gDwx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "\n",
        "def upload_to_s3(X, y, channel_name):\n",
        "    # Stack target (y) as the first column, followed by features (X)\n",
        "    data = np.column_stack((y, X))\n",
        "\n",
        "    # Write to a CSV buffer (in-memory)\n",
        "    csv_buffer = io.BytesIO()\n",
        "    np.savetxt(csv_buffer, data, delimiter=',', fmt='%g')\n",
        "\n",
        "    # Construct S3 Key\n",
        "    s3_key = f\"{prefix}/{channel_name}/data.csv\"\n",
        "\n",
        "    # Upload\n",
        "    boto3.resource('s3').Bucket(bucket).put_object(Key=s3_key, Body=csv_buffer.getvalue())\n",
        "    s3_uri = f\"s3://{bucket}/{s3_key}\"\n",
        "    print(f\"Uploaded {channel_name} data to: {s3_uri}\")\n",
        "    return s3_uri\n",
        "\n",
        "# Upload Train and Validation sets\n",
        "s3_train_uri = upload_to_s3(X_train, y_train, 'train')\n",
        "s3_val_uri = upload_to_s3(X_val, y_val, 'validation')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "pS59g3loDwx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 4. Training: AWS Linear Learner\n",
        "\n",
        "We will now launch a **Training Job**. This happens on a separate EC2 instance managed by SageMaker, not in this notebook.\n",
        "\n",
        "### 4.1 Define the Estimator\n",
        "\n",
        "We use the `sagemaker.image_uris` to get the Docker container for the Linear Learner algorithm."
      ],
      "metadata": {
        "id": "inwFcqvLDwyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.image_uris import retrieve\n",
        "\n",
        "# 1. Retrieve the container image\n",
        "container = retrieve('linear-learner', region)\n",
        "\n",
        "# 2. Define the Estimator (The configuration for the training job)\n",
        "ll_estimator = sagemaker.estimator.Estimator(\n",
        "    image_uri=container,\n",
        "    role=role,\n",
        "    instance_count=1,\n",
        "    instance_type='ml.m5.large', # Balanced general purpose instance\n",
        "    output_path=f\"s3://{bucket}/{prefix}/output\",\n",
        "    sagemaker_session=session\n",
        ")\n",
        "\n",
        "# 3. Set Hyperparameters\n",
        "ll_estimator.set_hyperparameters(\n",
        "    feature_dim=X_train.shape[1], # Must match number of columns in X\n",
        "    predictor_type='regressor',   # Regression problem\n",
        "    mini_batch_size=100,\n",
        "    epochs=15,\n",
        "    normalize_data=False,         # We already scaled it\n",
        "    loss='squared_loss'           # Optimizing for MSE\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "QGs4Q1jADwyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Execute Training\n",
        "\n",
        "This block will output logs from the remote instance. Wait for it to complete (approx 3-4 minutes)."
      ],
      "metadata": {
        "id": "JmkecFhhDwyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data channels\n",
        "train_input = sagemaker.inputs.TrainingInput(s3_train_uri, content_type='text/csv')\n",
        "val_input = sagemaker.inputs.TrainingInput(s3_val_uri, content_type='text/csv')\n",
        "\n",
        "# Start the job\n",
        "ll_estimator.fit({'train': train_input, 'validation': val_input})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "srl2wveIDwyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 5. Deployment & Inference\n",
        "\n",
        "Once training is successful, we deploy the model to an **Endpoint**. This creates a persistent REST API that we can query."
      ],
      "metadata": {
        "id": "uSOXqsRFDwyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.serializers import CSVSerializer\n",
        "from sagemaker.deserializers import JSONDeserializer\n",
        "\n",
        "# Deploy (approx 3-5 mins)\n",
        "predictor = ll_estimator.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type='ml.t2.medium', # Cost-effective for lab inference\n",
        "    serializer=CSVSerializer(),   # Converts list -> CSV for the endpoint\n",
        "    deserializer=JSONDeserializer() # Parses JSON response from endpoint\n",
        ")\n",
        "\n",
        "print(f\"Endpoint deployed: {predictor.endpoint_name}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qF9D1-PcDwyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Testing the Prediction\n",
        "\n",
        "Let's send a single record from our test set to the endpoint to see the result."
      ],
      "metadata": {
        "id": "wRCI0T2aDwyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample\n",
        "sample_input = X_test[0]\n",
        "actual_value = y_test.iloc[0]\n",
        "\n",
        "# Query the endpoint\n",
        "response = predictor.predict(sample_input)\n",
        "\n",
        "# Extract prediction\n",
        "predicted_value = response['predictions'][0]['score']\n",
        "\n",
        "print(f\"Actual Value:    ${actual_value * 100000:,.2f}\")\n",
        "print(f\"Predicted Value: ${predicted_value * 100000:,.2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "natn8I9WDwyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 6. Benchmarking: SageMaker vs. Scikit-Learn\n",
        "\n",
        "To understand if our model is \"good,\" we compare it to a reference implementation running locally in the notebook.\n",
        "\n",
        "### 6.1 Scikit-Learn Reference (Local)"
      ],
      "metadata": {
        "id": "rhNtWLkGDwyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Train Local Model\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict Local\n",
        "y_pred_sklearn = lr.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse_sklearn = np.sqrt(mean_squared_error(y_test, y_pred_sklearn))\n",
        "print(f\"Scikit-Learn RMSE: {rmse_sklearn:.4f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "cLhbebanDwyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 SageMaker Evaluation (Remote)\n",
        "\n",
        "We batch predict the entire test set using the endpoint."
      ],
      "metadata": {
        "id": "apiCkM-EDwyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_batch(data, predictor, rows=500):\n",
        "    # Split data into chunks to respect payload limits\n",
        "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
        "    predictions = []\n",
        "    for array in split_array:\n",
        "        result = predictor.predict(array)\n",
        "        predictions += [r['score'] for r in result['predictions']]\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Get all predictions\n",
        "y_pred_aws = predict_batch(X_test, predictor)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse_aws = np.sqrt(mean_squared_error(y_test, y_pred_aws))\n",
        "\n",
        "print(\"------ RESULTS ------\")\n",
        "print(f\"AWS Linear Learner RMSE: {rmse_aws:.4f}\")\n",
        "print(f\"Scikit-Learn RMSE:       {rmse_sklearn:.4f}\")\n",
        "print(\"---------------------\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "3S2jmyxMDwyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: The results should be very similar as both use linear regression techniques. Slight differences arise from optimization solvers (SGD vs OLS) and regularization defaults.*\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Cleanup\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT:** Delete the endpoint to stop billing."
      ],
      "metadata": {
        "id": "pkJ-7m1aDwyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete endpoint\n",
        "predictor.delete_endpoint()\n",
        "\n",
        "# Optional: Delete the model and endpoint configuration if you want a clean slate\n",
        "# predictor.delete_model()\n",
        "# predictor.delete_endpoint_config()\n",
        "\n",
        "print(\"Endpoint deleted. Lab complete.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ci3pWbqnDwyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SageMaker Canvas** is a \"no-code\" machine learning interface that allows you to build the same regression model (predicting housing prices) without writing a single line of Python. It uses the same AutoML technology (Amazon SageMaker Autopilot) under the hood.\n",
        "\n",
        "Here is the **No-Code Lab** walkthrough for the California Housing dataset.\n",
        "\n",
        "---\n",
        "\n",
        "# üé® Lab: California Housing Prediction (No-Code Edition)\n",
        "\n",
        "**Role:** Business Analyst / Citizen Data Scientist\n",
        "**Tool:** Amazon SageMaker Canvas\n",
        "**Objective:** Build a regression model to predict `MedHouseVal` using a visual drag-and-drop interface.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Preparation\n",
        "\n",
        "Before starting, ensure you have the dataset file on your local computer.\n",
        "\n",
        "* **Download:** If you don't have it, you can download the `housing.csv` from the [standard repository](https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv) or export it from your previous notebook using `df.to_csv('housing.csv', index=False)`.\n",
        "\n",
        "## 2. Launch SageMaker Canvas\n",
        "\n",
        "1. Open the **AWS Console** and navigate to **Amazon SageMaker**.\n",
        "2. In the left sidebar, select **Canvas**.\n",
        "3. Select your User Profile and click **Open Canvas**. (This may take 1-2 minutes to initialize).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Step-by-Step Walkthrough\n",
        "\n",
        "### Step 1: Import Data\n",
        "\n",
        "We need to load the California Housing CSV into Canvas.\n",
        "\n",
        "1. On the left sidebar, click the **Datasets** icon (database symbol).\n",
        "2. Click **Import** (top right).\n",
        "3. Choose **Upload** (from your computer) and select your `housing.csv` file.\n",
        "4. Click **Preview** to verify the columns (`Longitude`, `Latitude`, `MedHouseVal`, etc.) look correct.\n",
        "5. Click **Import Data**.\n",
        "\n",
        "### Step 2: Build the Model\n",
        "\n",
        "Now we define what we want to predict.\n",
        "\n",
        "1. Go to the **My models** tab (model symbol) on the left sidebar.\n",
        "2. Click **New model** and name it `CaliforniaHousing-Regression`.\n",
        "3. Select the **Predictive Analysis** radio button and click **Create**.\n",
        "4. **Select Dataset:** Choose the `housing.csv` you just imported and click **Select dataset**.\n",
        "\n",
        "### Step 3: Configure Target & Model Type\n",
        "\n",
        "1. **Select Target Column:** In the \"Target column\" dropdown, select **`MedHouseVal`** (Median House Value).\n",
        "2. **Verify Model Type:** Canvas will automatically detect this is a **Numeric Prediction** (Regression) problem.\n",
        "3. **Data Preview:** You will see a distribution histogram of house prices.\n",
        "4. **Build Options:** You have two choices:\n",
        "* **Quick Build:** Takes 2-15 minutes. Good for rapid prototyping. (Recommended for this lab).\n",
        "* **Standard Build:** Takes 1-2 hours. detailed analysis and higher accuracy.\n",
        "\n",
        "\n",
        "5. Click **Quick Build**.\n",
        "\n",
        "### Step 4: Analyze Results\n",
        "\n",
        "Once the build is complete (approx. 10 mins), Canvas presents a visual dashboard.\n",
        "\n",
        "1. **Overview Tab:** Look at the **RMSE** (Root Mean Squared Error).\n",
        "* *Compare this number to the Python notebook result. It usually achieves similar performance.*\n",
        "\n",
        "\n",
        "2. **Column Impact:** This is the equivalent of \"Feature Importance.\"\n",
        "* You will likely see **`MedInc`** (Median Income) having the highest percentage impact, consistent with our Python analysis.\n",
        "* You can click on a column (e.g., `Latitude`) to see how changing its value impacts the predicted price (Partial Dependence Plots).\n",
        "\n",
        "\n",
        "\n",
        "### Step 5: Generate Predictions\n",
        "\n",
        "1. Click the **Predict** button at the bottom of the analysis page.\n",
        "2. **Single Prediction (What-if analysis):**\n",
        "* Manually adjust sliders for `MedInc` or `AveRooms` to see how the predicted House Value changes in real-time.\n",
        "\n",
        "\n",
        "3. **Batch Prediction:**\n",
        "* If you have a separate test file (without targets), you can upload it here. Canvas will generate a CSV with predictions for every row.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Summary: Code vs. Canvas\n",
        "\n",
        "| Feature | SageMaker Notebook (Python) | SageMaker Canvas (No-Code) |\n",
        "| --- | --- | --- |\n",
        "| **User** | Data Scientist / ML Engineer | Business Analyst / Domain Expert |\n",
        "| **Flexibility** | High (Custom feature engineering) | Moderate (Auto-inferred) |\n",
        "| **Speed** | Slow (requires coding/debugging) | Fast (Click & Go) |\n",
        "| **Under the hood** | You choose (XGBoost, Linear Learner) | AutoML (Ensemble of models) |\n",
        "\n",
        "**Conclusion:** Canvas is excellent for establishing a \"baseline\" performance or allowing non-coders to validate hypotheses before a data scientist builds a production pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### Video Tutorial\n",
        "\n",
        "For a visual guide on building a regression model in Canvas, watch this video:\n",
        "[Build a Regression Model in 11 minutes with Amazon Sagemaker Canvas](https://www.youtube.com/watch?v=o_vPaVQ8D1o)\n",
        "\n",
        "This video demonstrates a similar workflow using a different dataset, but the steps for selecting the target column and interpreting the column impact are identical to what you will do with the housing data."
      ],
      "metadata": {
        "id": "mhl7qFZZDwyN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6gWqLaFEdic"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
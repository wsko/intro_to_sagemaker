{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56f2d7-Nshhf"
   },
   "source": [
    "Hello\\! As an AWS Solutions Architect, I’ve designed this workflow to be streamlined and cost-effective, leveraging the serverless nature of **Amazon Athena** and **SageMaker AI**.\n",
    "\n",
    "Since you are running this as an admin, you already have the necessary IAM permissions to create buckets and run crawlers. Here is your step-by-step implementation guide.\n",
    "\n",
    "-----\n",
    "\n",
    "## Phase 1: Data Lake Setup (S3 & Glue)\n",
    "\n",
    "### 1\\. Create the S3 Bucket\n",
    "\n",
    "  * Navigate to the **S3 Console**.\n",
    "  * Click **Create bucket**.\n",
    "  * **Bucket name:** `olist-analytics-data-[your-account-id]` (must be globally unique).\n",
    "  * Keep default settings (Block all public access should be **on**).\n",
    "\n",
    "### 2\\. Upload OLIST CSV Files\n",
    "\n",
    "  * Organize your local OLIST files into folders if possible (e.g., `orders/`, `products/`, `customers/`).\n",
    "  * In the S3 console, enter your bucket and click **Upload**.\n",
    "  * Drag and drop the CSV files.\n",
    "    > **Architect's Tip:** Ensure your folder structure is clean, as the Glue Crawler will use the folder names to define the table names in Athena.\n",
    "\n",
    "### 3\\. Create Athena Database via Glue Crawler\n",
    "\n",
    "We will use a Crawler to automatically infer the schema from your CSVs.\n",
    "\n",
    "  * Go to **AWS Glue Console** -\\> **Crawlers** -\\> **Create crawler**.\n",
    "  * **Name:** `olist-crawler`.\n",
    "  * **Data source:** Add your S3 bucket path (e.g., `s3://olist-analytics-data-.../`).\n",
    "  * **IAM Role:** Create a new role (Glue will auto-generate one for you).\n",
    "  * **Output Database:** Click **Add database** and name it `olist_db`.\n",
    "  * **Schedule:** Set to **On demand**.\n",
    "  * **Run Crawler:** Once created, select the crawler and click **Run**. This will populate the `olist_db` with tables corresponding to your CSVs.\n",
    "\n",
    "-----\n",
    "\n",
    "## Phase 2: Analytics Environment (SageMaker AI)\n",
    "\n",
    "### 4\\. Open a SageMaker Serverless Notebook\n",
    "\n",
    "AWS has transitioned the notebook experience. We will use the **SageMaker AI** (formerly SageMaker Studio) interface.\n",
    "\n",
    "1.  Navigate to **Amazon SageMaker** in the AWS Console.\n",
    "2.  Click on **SageMaker AI** (or Studio) in the left sidebar.\n",
    "3.  Under the **Applications** section, select **JupyterLab**.\n",
    "4.  Create or Open a **Space**.\n",
    "5.  Inside the JupyterLab interface, you are running in a managed, serverless environment. Simply open a new `.ipynb` file using the **Python 3 (Data Science)** kernel.\n",
    "\n",
    "-----\n",
    "\n",
    "## Phase 3: Notebook Implementation\n",
    "\n",
    "### 5\\. Starter Code: Connection & Configuration\n",
    "\n",
    "You will need the `PyAthena` library. If it's not installed in your environment, run `!pip install pyathena` in the first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yu6NUQiSshht"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyathena import connect\n",
    "\n",
    "# Configuration\n",
    "REGION = 'us-east-1' # Change to your region\n",
    "S3_STAGING_DIR = 's3://your-athena-query-results-bucket/' # Athena needs a place to store query metadata\n",
    "DATABASE = 'olist_db'\n",
    "\n",
    "# Establish Connection\n",
    "conn = connect(s3_staging_dir=S3_STAGING_DIR,\n",
    "               region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKPEU6okshhx"
   },
   "source": [
    "### 6\\. Sample SQL Queries (Group By & Joins)\n",
    "\n",
    "These queries help you understand order volume and customer distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wd8ciyJ9shhz"
   },
   "outputs": [],
   "source": [
    "# Query 1: Top 5 Cities by Order Count (Group By)\n",
    "query_1 = f\"\"\"\n",
    "SELECT\n",
    "    customer_city,\n",
    "    count(customer_id) as total_orders\n",
    "FROM \"{DATABASE}\".\"customers\"\n",
    "GROUP BY customer_city\n",
    "ORDER BY total_orders DESC\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "# Query 2: Joining Orders and Payments to find Revenue by Payment Type (Join)\n",
    "query_2 = f\"\"\"\n",
    "SELECT\n",
    "    p.payment_type,\n",
    "    SUM(p.payment_value) as total_revenue\n",
    "FROM \"{DATABASE}\".\"order_payments\" p\n",
    "JOIN \"{DATABASE}\".\"orders\" o ON p.order_id = o.order_id\n",
    "WHERE o.order_status = 'delivered'\n",
    "GROUP BY p.payment_type\n",
    "ORDER BY total_revenue DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69FM5Xt5shh0"
   },
   "source": [
    "### 7\\. Import Results into Pandas DataFrame\n",
    "\n",
    "The most efficient way to handle this in SageMaker is to use the `pd.read_sql` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCjBH3yoshh1"
   },
   "outputs": [],
   "source": [
    "# Function to execute and return DataFrame\n",
    "def get_athena_data(query):\n",
    "    return pd.read_sql(query, conn)\n",
    "\n",
    "# Load Top Cities into DF\n",
    "df_cities = get_athena_data(query_1)\n",
    "\n",
    "# Load Revenue Analysis into DF\n",
    "df_revenue = get_athena_data(query_2)\n",
    "\n",
    "# Display results\n",
    "print(\"Top Cities Analysis:\")\n",
    "print(df_cities.head())\n",
    "\n",
    "print(\"\\nRevenue by Payment Type:\")\n",
    "print(df_revenue.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "arn:aws:iam::254283132378:role/service-role/AmazonSageMaker-ExecutionRole-20251215T191125\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.get_execution_role())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy get_athena_dataget_athena_data the Role Name: From your error message, the role is AmazonSageMaker-ExecutionRole-20251215T191125.\n",
    "\n",
    "Go to IAM Console: Navigate to Roles and search for that name.\n",
    "\n",
    "Add Permissions: * Click Add permissions -> Attach policies.\n",
    "\n",
    "Search for and attach the managed policy: AWSGlueConsoleFullAccess.\n",
    "\n",
    "Note: This unblocks glue:GetDatabase, glue:GetTable, and other metadata actions Athena requires.\n",
    "\n",
    "2. Grant the Permission in IAM\n",
    "Go to the IAM Console in a new tab.\n",
    "\n",
    "Click Roles on the left and search for the role name you just copied.\n",
    "\n",
    "Click on the Role name.\n",
    "\n",
    "Click Add permissions -> Attach policies.\n",
    "\n",
    "Since you are an admin and likely want to move fast, search for AmazonS3FullAccess and check the box.\n",
    "\n",
    "Architect's Note: In a production environment, we'd use \"Least Privilege\" (only the specific bucket), but for an OLIST sandbox with an admin user, AmazonS3FullAccess is the fastest way to unblock your workflow.\n",
    "\n",
    "Also, search for and attach AmazonAthenaFullAccess just to be safe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n2hrs2vs-YF"
   },
   "source": [
    "Use **SQL Magic commands**. This is often preferred by Data Engineers because it allows you to write raw SQL in a cell without wrapping it in Python strings, providing better syntax highlighting and readability.\n",
    "\n",
    "To do this, we use the `ipython-sql` extension and the `PyAthena` driver.\n",
    "\n",
    "### 1\\. Install and Load the Extension\n",
    "\n",
    "Run this in your first notebook cell to set up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TUaXhusFs-YI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install ipython-sql pyathena --quiet\n",
    "\n",
    "# Load the SQL magic extension\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNofZ7Urs-YJ"
   },
   "source": [
    "### 2\\. Connect to Athena\n",
    "\n",
    "Instead of a standard Python connection object, you will pass a connection string to the `%sql` magic command. Replace the placeholders with your specific S3 bucket (created in Step 1) and your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RdHysvKPs-YK"
   },
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "# Configuration\n",
    "region = \"us-east-1\"\n",
    "athena_db = \"olist_db\"\n",
    "# S3 path for Athena to store query results (must end with a /)\n",
    "s3_output = \"s3://olist-analytics-data-results/output/\"\n",
    "\n",
    "# Encode the S3 path for the connection string\n",
    "connection_string = f\"awsathena+rest://@athena.{region}.amazonaws.com/{athena_db}?s3_staging_dir={urllib.parse.quote_plus(s3_output)}\"\n",
    "\n",
    "# Connect the magic command to Athena\n",
    "%sql $connection_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the PrettyTable version conflict\n",
    "%config SqlMagic.style = '_DEPRECATED_DEFAULT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * awsathena+rest://@athena.us-east-1.amazonaws.com/olist_db?s3_staging_dir=s3%3A%2F%2Folist-analytics-data-results%2Foutput%2F\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>tab_name</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>customers</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>geolocation</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>order_items</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>order_payments</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>order_reviews</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>orders</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>product_category_name_translation</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>products</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>sellers</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[('customers',),\n",
       " ('geolocation',),\n",
       " ('order_items',),\n",
       " ('order_payments',),\n",
       " ('order_reviews',),\n",
       " ('orders',),\n",
       " ('product_category_name_translation',),\n",
       " ('products',),\n",
       " ('sellers',)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SHOW TABLES IN olist_db;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%sql\n",
    "#DESCRIBE order_items;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>order_id</th>\n",
       "            <th>order_item_id</th>\n",
       "            <th>product_id</th>\n",
       "            <th>seller_id</th>\n",
       "            <th>shipping_limit_date</th>\n",
       "            <th>price</th>\n",
       "            <th>freight_value</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>&quot;00010242fe8c5a6d1ba2dd792cb16214&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>&quot;4244733e06e7ecb4970a6e2683c13e61&quot;</td>\n",
       "            <td>&quot;48436dade18ac8b2bce089ec2a041202&quot;</td>\n",
       "            <td>2017-09-19 09:45:35</td>\n",
       "            <td>58.9</td>\n",
       "            <td>13.29</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;00018f77f2f0320c557190d7a144bdd3&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "            <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "            <td>2017-05-03 11:05:13</td>\n",
       "            <td>239.9</td>\n",
       "            <td>19.93</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;000229ec398224ef6ca0657da4fc703e&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "            <td>&quot;5b51032eddd242adc84c38acab88f23d&quot;</td>\n",
       "            <td>2018-01-18 14:48:30</td>\n",
       "            <td>199.0</td>\n",
       "            <td>17.87</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;00024acbcdf0a6daa1e931b038114c75&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>&quot;7634da152a4610f1595efa32f14722fc&quot;</td>\n",
       "            <td>&quot;9d7a1d34a5052409006425275ba1c2b4&quot;</td>\n",
       "            <td>2018-08-15 10:10:18</td>\n",
       "            <td>12.99</td>\n",
       "            <td>12.79</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;00042b26cf59d7ce69dfabb4e55b4fd9&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>ac6c3623068f30de03045865e4e10089</td>\n",
       "            <td>df560393f3a51e74553ab94004ba5c87</td>\n",
       "            <td>2017-02-13 13:57:51</td>\n",
       "            <td>199.9</td>\n",
       "            <td>18.14</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;00048cc3ae777c65dbb7d2a0634bc1ea&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>ef92defde845ab8450f9d70c526ef70f</td>\n",
       "            <td>&quot;6426d21aca402a131fc0a5d0960a3c90&quot;</td>\n",
       "            <td>2017-05-23 03:55:27</td>\n",
       "            <td>21.9</td>\n",
       "            <td>12.69</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;00054e8431b9d7675808bcb819fb4a32&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>&quot;8d4f2bb7e93e6710a28f34fa83ee7d28&quot;</td>\n",
       "            <td>&quot;7040e82f899a04d1b434b795a43b4617&quot;</td>\n",
       "            <td>2017-12-14 12:10:31</td>\n",
       "            <td>19.9</td>\n",
       "            <td>11.85</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;000576fe39319847cbb9d288c5617fa6&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>&quot;557d850972a7d6f792fd18ae1400d9b6&quot;</td>\n",
       "            <td>&quot;5996cddab893a4652a15592fb58ab8db&quot;</td>\n",
       "            <td>2018-07-10 12:30:45</td>\n",
       "            <td>810.0</td>\n",
       "            <td>70.75</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;0005a1a1728c9d785b8e2b08b904576c&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>&quot;310ae3c140ff94b03219ad0adc3c778f&quot;</td>\n",
       "            <td>a416b6a846a11724393025641d4edd5e</td>\n",
       "            <td>2018-03-26 18:31:29</td>\n",
       "            <td>145.95</td>\n",
       "            <td>11.65</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>&quot;0005f50442cb953dcd1d21e1fb923495&quot;</td>\n",
       "            <td>1</td>\n",
       "            <td>&quot;4535b0e1091c278dfd193e5a1d63b39f&quot;</td>\n",
       "            <td>ba143b05f0110f0dc71ad71b4466ce92</td>\n",
       "            <td>2018-07-06 14:10:56</td>\n",
       "            <td>53.99</td>\n",
       "            <td>11.4</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "[('\"00010242fe8c5a6d1ba2dd792cb16214\"', 1, '\"4244733e06e7ecb4970a6e2683c13e61\"', '\"48436dade18ac8b2bce089ec2a041202\"', '2017-09-19 09:45:35', 58.9, 13.29),\n",
       " ('\"00018f77f2f0320c557190d7a144bdd3\"', 1, 'e5f2d52b802189ee658865ca93d83a8f', 'dd7ddc04e1b6c2c614352b383efe2d36', '2017-05-03 11:05:13', 239.9, 19.93),\n",
       " ('\"000229ec398224ef6ca0657da4fc703e\"', 1, 'c777355d18b72b67abbeef9df44fd0fd', '\"5b51032eddd242adc84c38acab88f23d\"', '2018-01-18 14:48:30', 199.0, 17.87),\n",
       " ('\"00024acbcdf0a6daa1e931b038114c75\"', 1, '\"7634da152a4610f1595efa32f14722fc\"', '\"9d7a1d34a5052409006425275ba1c2b4\"', '2018-08-15 10:10:18', 12.99, 12.79),\n",
       " ('\"00042b26cf59d7ce69dfabb4e55b4fd9\"', 1, 'ac6c3623068f30de03045865e4e10089', 'df560393f3a51e74553ab94004ba5c87', '2017-02-13 13:57:51', 199.9, 18.14),\n",
       " ('\"00048cc3ae777c65dbb7d2a0634bc1ea\"', 1, 'ef92defde845ab8450f9d70c526ef70f', '\"6426d21aca402a131fc0a5d0960a3c90\"', '2017-05-23 03:55:27', 21.9, 12.69),\n",
       " ('\"00054e8431b9d7675808bcb819fb4a32\"', 1, '\"8d4f2bb7e93e6710a28f34fa83ee7d28\"', '\"7040e82f899a04d1b434b795a43b4617\"', '2017-12-14 12:10:31', 19.9, 11.85),\n",
       " ('\"000576fe39319847cbb9d288c5617fa6\"', 1, '\"557d850972a7d6f792fd18ae1400d9b6\"', '\"5996cddab893a4652a15592fb58ab8db\"', '2018-07-10 12:30:45', 810.0, 70.75),\n",
       " ('\"0005a1a1728c9d785b8e2b08b904576c\"', 1, '\"310ae3c140ff94b03219ad0adc3c778f\"', 'a416b6a846a11724393025641d4edd5e', '2018-03-26 18:31:29', 145.95, 11.65),\n",
       " ('\"0005f50442cb953dcd1d21e1fb923495\"', 1, '\"4535b0e1091c278dfd193e5a1d63b39f\"', 'ba143b05f0110f0dc71ad71b4466ce92', '2018-07-06 14:10:56', 53.99, 11.4)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from order_items\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/* Find the top 5 product categories by number of items sold */\n",
    "SELECT\n",
    "    product_category_name,\n",
    "    COUNT(*) as items_sold\n",
    "FROM order_items\n",
    "GROUP BY product_category_name\n",
    "ORDER BY items_sold DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsTQqmzRs-YL"
   },
   "source": [
    "-----\n",
    "\n",
    "### 3\\. Run SQL Queries Directly in Cells\n",
    "\n",
    "Now you can use the `%%sql` prefix at the top of any cell to write pure SQL.\n",
    "\n",
    "**Example: Group By Analysis**\n",
    "\n",
    "```sql\n",
    "%%sql\n",
    "/* Find the top 5 product categories by number of items sold */\n",
    "SELECT\n",
    "    product_category_name,\n",
    "    COUNT(*) as items_sold\n",
    "FROM olist_order_items_dataset\n",
    "GROUP BY product_category_name\n",
    "ORDER BY items_sold DESC\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "**Example: Multi-Table Join**\n",
    "\n",
    "```sql\n",
    "%%sql\n",
    "/* Join orders and customers to see the delivery performance by state */\n",
    "SELECT\n",
    "    c.customer_state,\n",
    "    AVG(date_diff('day', CAST(o.order_purchase_timestamp AS TIMESTAMP), CAST(o.order_delivered_customer_date AS TIMESTAMP))) AS avg_delivery_days\n",
    "FROM olist_orders_dataset o\n",
    "JOIN olist_customers_dataset c ON o.customer_id = c.customer_id\n",
    "WHERE o.order_status = 'delivered'\n",
    "GROUP BY c.customer_state\n",
    "ORDER BY avg_delivery_days ASC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "Since we are looking at the OLIST dataset—which is a treasure trove of Brazilian e-commerce history—we can dig into more complex \"business intelligence\" style queries.\n",
    "\n",
    "Here are three advanced SQL queries using **Window Functions**, **Date Arithmetic**, and **Complex Joins** to run directly in your SageMaker SQL cells.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Customer Retention: Repeat Purchase Behavior\n",
    "\n",
    "This query identifies \"Loyalists\" by calculating how many customers have placed more than one order. It requires joining the `orders` table with the `customers` table since `customer_id` in the orders table is unique per order, but `customer_unique_id` is the actual permanent ID for the person.\n",
    "\n",
    "```sql\n",
    "%%sql\n",
    "SELECT\n",
    "    CASE\n",
    "        WHEN order_count = 1 THEN 'One-time Buyer'\n",
    "        WHEN order_count = 2 THEN 'Repeat Buyer (2)'\n",
    "        ELSE 'Loyal Customer (3+)'\n",
    "    END AS customer_segment,\n",
    "    COUNT(*) as customer_count\n",
    "FROM (\n",
    "    SELECT\n",
    "        c.customer_unique_id,\n",
    "        COUNT(o.order_id) as order_count\n",
    "    FROM olist_orders_dataset o\n",
    "    JOIN olist_customers_dataset c ON o.customer_id = c.customer_id\n",
    "    GROUP BY c.customer_unique_id\n",
    ")\n",
    "GROUP BY 1\n",
    "ORDER BY customer_count DESC;\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Seasonality: Monthly Revenue Growth (%)\n",
    "\n",
    "This is a more advanced query using the **Window Function** `LAG()` to compare the current month's revenue to the previous month. This is a classic \"Solution Architect\" pattern for building executive dashboards.\n",
    "\n",
    "```sql\n",
    "%%sql\n",
    "WITH MonthlyRevenue AS (\n",
    "    SELECT\n",
    "        date_format(CAST(o.order_purchase_timestamp AS TIMESTAMP), '%Y-%m') AS order_month,\n",
    "        SUM(p.payment_value) AS revenue\n",
    "    FROM olist_orders_dataset o\n",
    "    JOIN olist_order_payments_dataset p ON o.order_id = p.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "    GROUP BY 1\n",
    ")\n",
    "SELECT\n",
    "    order_month,\n",
    "    revenue,\n",
    "    LAG(revenue) OVER (ORDER BY order_month) AS prev_month_revenue,\n",
    "    (revenue - LAG(revenue) OVER (ORDER BY order_month)) / LAG(revenue) OVER (ORDER BY order_month) * 100 AS pct_growth\n",
    "FROM MonthlyRevenue\n",
    "ORDER BY order_month;\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Logistic Bottlenecks: Estimated vs. Actual Delivery\n",
    "\n",
    "One of the biggest challenges in Brazilian e-commerce is geography. This query finds the states where the logistics gap (Difference between when the customer was *told* it would arrive vs. when it *actually* arrived) is the highest.\n",
    "\n",
    "```sql\n",
    "%%sql\n",
    "SELECT\n",
    "    c.customer_state,\n",
    "    COUNT(o.order_id) as total_orders,\n",
    "    AVG(date_diff('day', CAST(o.order_delivered_customer_date AS TIMESTAMP), CAST(o.order_estimated_delivery_date AS TIMESTAMP))) AS days_ahead_of_schedule,\n",
    "    SUM(CASE WHEN o.order_delivered_customer_date > o.order_estimated_delivery_date THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_late\n",
    "FROM olist_orders_dataset o\n",
    "JOIN olist_customers_dataset c ON o.customer_id = c.customer_id\n",
    "WHERE o.order_status = 'delivered'\n",
    "  AND o.order_delivered_customer_date IS NOT NULL\n",
    "GROUP BY c.customer_state\n",
    "HAVING COUNT(o.order_id) > 100\n",
    "ORDER BY percent_late DESC;\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Tips for Athena Performance in SageMaker\n",
    "\n",
    "* **Case Sensitivity:** Athena tables and columns are usually lowercase. If your CSV headers had caps, the Glue Crawler might have lowercased them automatically.\n",
    "* **Date Casting:** OLIST timestamps are often stored as strings in CSVs. Notice I used `CAST(column AS TIMESTAMP)` in the queries above; this is essential for `date_diff` or `date_format` to work.\n",
    "\n",
    "**Next Step:** Would you like to see how to use **SageMaker Clarify** on these results to check for bias in your delivery times across different Brazilian states?\n",
    "-----\n",
    "\n",
    "\n",
    "### 4\\. Convert SQL Cell Results to Pandas\n",
    "\n",
    "One of the best features of SQL magic is that you can \"capture\" the output of a SQL cell directly into a Pandas DataFrame using the assignment syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNIK9lQos-YL"
   },
   "outputs": [],
   "source": [
    "# Use the << operator to assign the result of the SQL query to a variable named 'df_performance'\n",
    "%%sql df_performance <<\n",
    "SELECT\n",
    "    order_status,\n",
    "    COUNT(*) as count\n",
    "FROM olist_orders_dataset\n",
    "GROUP BY order_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toExDPYMs-YM"
   },
   "outputs": [],
   "source": [
    "# Now 'df_performance' is a standard Pandas DataFrame\n",
    "print(type(df_performance))\n",
    "df_performance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbGgr8zZs-YM"
   },
   "source": [
    "-----\n",
    "\n",
    "**Next Step:** Would you like to see how to create a **SageMaker Feature Store** from these SQL results to prepare the data for an ML model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXllgwYhuFY6"
   },
   "source": [
    "As an AWS Solutions Architect, I recommend using **Seaborn** for visualizations in SageMaker because it handles Pandas DataFrames natively and produces \"production-ready\" aesthetics with very little code.\n",
    "\n",
    "Below is the code to pull the data from Athena into Pandas and generate three key business insights: **Revenue Trends**, **Payment Preferences**, and **Category Performance**.\n",
    "\n",
    "### 1\\. Setup and Library Imports\n",
    "\n",
    "Run this in a new cell to prepare your plotting environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7t1XHFDcuFY_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set the visual style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHPptInluFZB"
   },
   "source": [
    "-----\n",
    "\n",
    "### 2\\. Visualization 1: Monthly Revenue Trend (Growth Analysis)\n",
    "\n",
    "We will pull the monthly revenue to see how the business is scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l108sPonuFZC"
   },
   "outputs": [],
   "source": [
    "# Query data into a DataFrame\n",
    "df_revenue_trend = get_athena_data(f\"\"\"\n",
    "    SELECT\n",
    "        date_format(CAST(o.order_purchase_timestamp AS TIMESTAMP), '%Y-%m-01') AS month,\n",
    "        SUM(p.payment_value) AS total_revenue\n",
    "    FROM \"{DATABASE}\".\"olist_orders_dataset\" o\n",
    "    JOIN \"{DATABASE}\".\"olist_order_payments_dataset\" p ON o.order_id = p.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "    GROUP BY 1\n",
    "    ORDER BY 1\n",
    "\"\"\")\n",
    "\n",
    "# Convert month string to datetime for better plotting\n",
    "df_revenue_trend['month'] = pd.to_datetime(df_revenue_trend['month'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df_revenue_trend, x='month', y='total_revenue', marker='o', color='#2ecc71')\n",
    "plt.title('OLIST Monthly Revenue Trend (BRL)', fontsize=15)\n",
    "plt.xlabel('Order Month')\n",
    "plt.ylabel('Revenue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCJ18ck-uFZD"
   },
   "source": [
    "-----\n",
    "\n",
    "### 3\\. Visualization 2: Payment Method Distribution\n",
    "\n",
    "This helps identify the most popular financial rails in the Brazilian market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhehCNkBuFZE"
   },
   "outputs": [],
   "source": [
    "# Query data\n",
    "df_payments = get_athena_data(f\"\"\"\n",
    "    SELECT payment_type, COUNT(*) as count\n",
    "    FROM \"{DATABASE}\".\"olist_order_payments_dataset\"\n",
    "    GROUP BY payment_type\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Plotting a Pie Chart (better for distribution)\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "plt.pie(df_payments['count'], labels=df_payments['payment_type'], autopct='%1.1f%%', colors=colors, startangle=140)\n",
    "plt.title('Customer Payment Method Distribution', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrW_pVVMuFZF"
   },
   "source": [
    "-----\n",
    "\n",
    "### 4\\. Visualization 3: Top 10 Product Categories by Revenue\n",
    "\n",
    "This join-heavy query identifies which categories drive the most value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAY9UuX1uFZF"
   },
   "outputs": [],
   "source": [
    "# Query data\n",
    "df_top_categories = get_athena_data(f\"\"\"\n",
    "    SELECT\n",
    "        p.product_category_name,\n",
    "        SUM(i.price) as total_sales\n",
    "    FROM \"{DATABASE}\".\"olist_order_items_dataset\" i\n",
    "    JOIN \"{DATABASE}\".\"olist_products_dataset\" p ON i.product_id = p.product_id\n",
    "    WHERE p.product_category_name IS NOT NULL\n",
    "    GROUP BY p.product_category_name\n",
    "    ORDER BY total_sales DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Plotting a Horizontal Bar Chart\n",
    "sns.barplot(data=df_top_categories, x='total_sales', y='product_category_name', palette='viridis')\n",
    "plt.title('Top 10 Product Categories by Revenue', fontsize=15)\n",
    "plt.xlabel('Total Sales (BRL)')\n",
    "plt.ylabel('Category Name')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDntu1_VuFZG"
   },
   "source": [
    "-----\n",
    "\n",
    "### 5\\. Architectural Tip: Handling \"Big Data\" in SageMaker\n",
    "\n",
    "If your OLIST dataset grows to millions of rows:\n",
    "\n",
    "1.  **Sampling:** Use `TABLESAMPLE BERNOULLI (10)` in your SQL query to pull only a 10% representative sample into Pandas to save memory.\n",
    "2.  **Parquet Conversion:** In Step 3 (Crawler), if you convert your CSVs to **Parquet** format in S3, your Athena queries will run up to **10x faster** and cost **90% less** because Athena only reads the columns needed for the visualization.\n",
    "\n",
    "**Next Step:** Would you like to build a **Linear Regression model** in this notebook to predict future delivery times based on the geographic data we just visualized?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
